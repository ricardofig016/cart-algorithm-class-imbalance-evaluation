{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCART Implementation for Class Imbalance Study\\n\\nOriginal Reference Implementation:\\n- Adapted from zziz/cart (https://github.com/zziz/cart)\\n- Clean-room implementation without direct code copying\\n\\nKey Modifications from Reference:\\n1. Architecture Simplification:\\n   - Removed regression functionality to focus purely on classification\\n   - Unified tree structure with dedicated TreeNode class\\n   - Simplified API (removed pruning parameters, consolidated initialization)\\n\\n2. Project-Specific Optimizations:\\n   - Direct compatibility with preprocessed numpy arrays (from data/processed/)\\n   - Binary classification focus with class label tracking\\n   - Early stopping criteria aligned with imbalance analysis needs\\n   - Memory-efficient node structure for large datasets\\n\\n3. Phase 2 Readiness:\\n   - Modular impurity calculations for weighted Gini modification\\n   - Class label preservation for imbalance weighting\\n   - Predict method optimized for probability-based metrics (ROC-AUC)\\n\\nImplementation Differences from Reference:\\n- No sklearn dependencies\\n- Removed print_tree visualization methods\\n- Simplified split criteria to essential parameters\\n- Vectorized impurity calculations for performance\\n- Added sample counting for imbalance analysis\\n\\nImportant Notes:\\n- Designed for binary classification (handles multi-class through majority voting)\\n- Requires preprocessed numerical features (compatible with utils/preprocess.py)\\n- Class labels stored in self.classes for Phase 2 modifications\\n\\nMaintains Core CART Functionality:\\n- Gini/Entropy split criteria\\n- Depth-based stopping\\n- Recursive tree construction\\n- Majority class prediction\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CART Implementation for Class Imbalance Study\n",
    "\n",
    "Original Reference Implementation:\n",
    "- Adapted from zziz/cart (https://github.com/zziz/cart)\n",
    "- Clean-room implementation without direct code copying\n",
    "\n",
    "Key Modifications from Reference:\n",
    "1. Architecture Simplification:\n",
    "   - Removed regression functionality to focus purely on classification\n",
    "   - Unified tree structure with dedicated TreeNode class\n",
    "   - Simplified API (removed pruning parameters, consolidated initialization)\n",
    "\n",
    "2. Project-Specific Optimizations:\n",
    "   - Direct compatibility with preprocessed numpy arrays (from data/processed/)\n",
    "   - Binary classification focus with class label tracking\n",
    "   - Early stopping criteria aligned with imbalance analysis needs\n",
    "   - Memory-efficient node structure for large datasets\n",
    "\n",
    "3. Phase 2 Readiness:\n",
    "   - Modular impurity calculations for weighted Gini modification\n",
    "   - Class label preservation for imbalance weighting\n",
    "   - Predict method optimized for probability-based metrics (ROC-AUC)\n",
    "\n",
    "Implementation Differences from Reference:\n",
    "- No sklearn dependencies\n",
    "- Removed print_tree visualization methods\n",
    "- Simplified split criteria to essential parameters\n",
    "- Vectorized impurity calculations for performance\n",
    "- Added sample counting for imbalance analysis\n",
    "\n",
    "Important Notes:\n",
    "- Designed for binary classification (handles multi-class through majority voting)\n",
    "- Requires preprocessed numerical features (compatible with utils/preprocess.py)\n",
    "- Class labels stored in self.classes for Phase 2 modifications\n",
    "\n",
    "Maintains Core CART Functionality:\n",
    "- Gini/Entropy split criteria\n",
    "- Depth-based stopping\n",
    "- Recursive tree construction\n",
    "- Majority class prediction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/class_imbalance:\n",
      "Processing dataset_978_mfeat-factors.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_947_arsenic-male-bladder.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1004_synthetic_control.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1056_mc1.csv...\n",
      "Classes: [np.False_, np.True_] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_940_water-treatment.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_950_arsenic-female-lung.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1014_analcatdata_dmft.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1039_hiva_agnostic.csv...\n",
      "Classes: [np.int64(-1), np.int64(1)] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1018_ipums_la_99-small.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1002_ipums_la_98-small.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_765_analcatdata_apnea2.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1016_vowel.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1045_kc1-top5.csv...\n",
      "Classes: ['DEF', 'NODEF'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1013_analcatdata_challenger.csv...\n",
      "Classes: [np.int64(0), np.int64(1)] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_450_analcatdata_lawsuit.csv...\n",
      "Classes: [np.int64(0), np.int64(1)] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_312_scene.csv...\n",
      "Classes: [np.int64(0), np.int64(1)] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_995_mfeat-zernike.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_764_analcatdata_apnea3.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_311_oil_spill.csv...\n",
      "Classes: [np.int64(-1), np.int64(1)] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_980_optdigits.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_968_analcatdata_birthday.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_987_collins.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_767_analcatdata_apnea1.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_875_analcatdata_chlamydia.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1061_ar4.csv...\n",
      "Classes: [np.False_, np.True_] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_962_mfeat-morphological.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_757_meta.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_951_arsenic-male-lung.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_958_segment.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1064_ar6.csv...\n",
      "Classes: [np.False_, np.True_] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1050_pc3.csv...\n",
      "Classes: [np.False_, np.True_] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_463_backache.csv...\n",
      "Classes: [np.int64(0), np.int64(1)] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1000_hypothyroid.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1049_pc4.csv...\n",
      "Classes: [np.False_, np.True_] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1023_soybean.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_316_yeast_ml8.csv...\n",
      "Classes: [np.int64(0), np.int64(1)] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_949_arsenic-female-bladder.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1022_mfeat-pixel.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_954_spectrometer.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_966_analcatdata_halloffame.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_971_mfeat-fourier.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1065_kc3.csv...\n",
      "Classes: [np.False_, np.True_] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1021_page-blocks.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1020_mfeat-karhunen.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_1059_ar1.csv...\n",
      "Classes: [np.False_, np.True_] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_867_visualizing_livestock.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_38_sick.csv...\n",
      "Classes: ['negative', 'sick'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_984_analcatdata_draft.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_865_analcatdata_neavote.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n",
      "data/processed/class_imbalance:\n",
      "Processing dataset_976_JapaneseVowels.csv...\n",
      "Classes: ['N', 'P'] → [np.int64(0), np.int64(1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "\n",
    "def preprocess_datasets(\n",
    "    raw_dir=\"data/raw/class_imbalance\",\n",
    "    processed_dir=\"data/processed/class_imbalance\",\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    imputation_strategy=\"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess datasets with robust label encoding and validation\n",
    "    \"\"\"\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    label_encoder = LabelEncoder()\n",
    "        \n",
    "    for filename in os.listdir(raw_dir):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue\n",
    "        \n",
    "        print(f\"{processed_dir}:\")\n",
    "        print(f\"Processing {filename}...\")\n",
    "        file_path = os.path.join(raw_dir, filename)\n",
    "\n",
    "        try:\n",
    "            # Load and validate data\n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.shape[1] < 2:\n",
    "                print(f\"Skipping {filename}: Insufficient columns\")\n",
    "                continue\n",
    "\n",
    "            # Separate features and target\n",
    "            X = df.iloc[:, :-1].copy()\n",
    "            y_raw = df.iloc[:, -1].copy()\n",
    "\n",
    "            # Clean and encode labels\n",
    "            valid_mask = y_raw.notna()\n",
    "            X, y_raw = X[valid_mask], y_raw[valid_mask]\n",
    "            y_raw_sorted = np.sort(y_raw)\n",
    "            if len(y_raw_sorted) == 0:\n",
    "                print(f\"Skipping {filename}: No valid targets\")\n",
    "                continue\n",
    "\n",
    "            # Convert labels to 0-indexed integers\n",
    "            y = pd.Series(label_encoder.fit_transform(y_raw_sorted), name=y_raw.name)\n",
    "            classes = label_encoder.classes_\n",
    "            if len(classes) < 2:\n",
    "                print(f\"Skipping {filename}: Only one class present\")\n",
    "                continue\n",
    "\n",
    "            # Handle missing features\n",
    "            if imputation_strategy == \"drop\":\n",
    "                valid_rows = X.notna().all(axis=1)\n",
    "                X, y = X[valid_rows], y[valid_rows]\n",
    "            else:\n",
    "                # Numerical imputation\n",
    "                num_cols = X.select_dtypes(include=np.number).columns\n",
    "                if len(num_cols) > 0:\n",
    "                    X[num_cols] = X[num_cols].fillna(X[num_cols].mean()).fillna(0)\n",
    "\n",
    "                # Categorical imputation\n",
    "                cat_cols = X.select_dtypes(exclude=np.number).columns\n",
    "                for col in cat_cols:\n",
    "                    mode = X[col].mode()[0] if not X[col].mode().empty else \"missing\"\n",
    "                    X[col] = X[col].fillna(mode)\n",
    "\n",
    "            # Convert categoricals to dummies\n",
    "            if len(cat_cols) > 0:\n",
    "                X = pd.get_dummies(X, columns=cat_cols, drop_first=False)\n",
    "\n",
    "            # Final validation\n",
    "            X = X.fillna(0).infer_objects()\n",
    "            assert not X.isna().any().any(), \"NaN values in features\"\n",
    "            assert X.shape[0] > 0, \"Empty dataset after preprocessing\"\n",
    "\n",
    "            # Normalize numericals\n",
    "            num_cols = X.select_dtypes(include=np.number).columns\n",
    "            if len(num_cols) > 0:\n",
    "                X[num_cols] = MinMaxScaler().fit_transform(X[num_cols])\n",
    "\n",
    "            # Stratified split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "\n",
    "            # Save processed data\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            output_dir = os.path.join(processed_dir, base_name)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            X_train.to_csv(os.path.join(output_dir, \"X_train.csv\"), index=False)\n",
    "            X_test.to_csv(os.path.join(output_dir, \"X_test.csv\"), index=False)\n",
    "            y_train.to_csv(os.path.join(output_dir, \"y_train.csv\"), index=False)\n",
    "            y_test.to_csv(os.path.join(output_dir, \"y_test.csv\"), index=False)\n",
    "\n",
    "            print(f\"Classes: {list(classes)} → {list(y.unique())}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed processing {filename}: {str(e)}\\n\")\n",
    "\n",
    "preprocess_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"Node structure for decision tree\"\"\"\n",
    "\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature  # Feature index for splitting\n",
    "        self.threshold = threshold  # Threshold value for split\n",
    "        self.left = left  # Left subtree\n",
    "        self.right = right  # Right subtree\n",
    "        self.value = value  # Class label for leaf nodes\n",
    "        self.samples = 0  # Number of samples in node\n",
    "        self.depth = 0  # Depth in tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"CART classifier with Gini/Entropy criteria for handling class imbalance analysis\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion=\"gini\"):\n",
    "        self.max_depth = max_depth  # Maximum tree depth\n",
    "        self.min_samples_split = min_samples_split  # Minimum samples to split\n",
    "        self.criterion = criterion.lower()  # Impurity measure (gini/entropy)\n",
    "        self.root = None  # Root node of decision tree\n",
    "        self.classes = None  # Store class labels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build decision tree from training data.\"\"\"\n",
    "        y = self._convert_labels(y)\n",
    "        self.classes = np.unique(y)\n",
    "        self.root = self._grow_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for input samples\"\"\"\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def _convert_labels(self, y):\n",
    "        \"\"\"Convert labels to np.int64. If conversion fails, map categorical labels to numbers.\"\"\"\n",
    "        try:\n",
    "            return y.astype(np.int64)\n",
    "        except ValueError:\n",
    "            uniques = np.unique(y)\n",
    "            mapping = {label: idx for idx, label in enumerate(uniques)}\n",
    "            return np.array([mapping[val] for val in y], dtype=np.int64)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build decision tree\"\"\"\n",
    "        node = TreeNode()\n",
    "        node.samples = X.shape[0]\n",
    "        node.depth = depth\n",
    "\n",
    "        # Stopping conditions\n",
    "        if (\n",
    "            (self.max_depth and depth >= self.max_depth)\n",
    "            or (node.samples < self.min_samples_split)\n",
    "            or (len(np.unique(y)) == 1)\n",
    "        ):\n",
    "            node.value = self._most_common(y)\n",
    "            return node\n",
    "\n",
    "        # Find optimal split\n",
    "        feature, threshold = self._best_split(X, y)\n",
    "        if feature is None:\n",
    "            node.value = self._most_common(y)\n",
    "            return node\n",
    "\n",
    "        # Split dataset\n",
    "        left_idx = X[:, feature] <= threshold\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        # Grow child nodes\n",
    "        node.feature = feature\n",
    "        node.threshold = threshold\n",
    "        node.left = self._grow_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "        node.right = self._grow_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find optimal feature and threshold for splitting\"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "        current_impurity = self._calculate_impurity(y)\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_y = y[X[:, feature] <= threshold]\n",
    "                right_y = y[X[:, feature] > threshold]\n",
    "\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = current_impurity - self._weighted_impurity(left_y, right_y)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _calculate_impurity(self, y):\n",
    "        \"\"\"Calculate impurity of target values\"\"\"\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "\n",
    "        if self.criterion == \"gini\":\n",
    "            return 1 - np.sum(proportions**2)\n",
    "        elif self.criterion == \"entropy\":\n",
    "            return -np.sum(proportions * np.log2(proportions))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion. Use 'gini' or 'entropy'\")\n",
    "\n",
    "    def _weighted_impurity(self, left_y, right_y):\n",
    "        \"\"\"Calculate weighted impurity for child nodes\"\"\"\n",
    "        n_left, n_right = len(left_y), len(right_y)\n",
    "        n_total = n_left + n_right\n",
    "\n",
    "        return (n_left / n_total) * self._calculate_impurity(left_y) + (\n",
    "            n_right / n_total\n",
    "        ) * self._calculate_impurity(right_y)\n",
    "\n",
    "    def _most_common(self, y):\n",
    "        \"\"\"Find majority class label\"\"\"\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def _traverse(self, x, node):\n",
    "        \"\"\"Traverse tree to make prediction\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse(x, node.left)\n",
    "        return self._traverse(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed processing dataset_978_mfeat-factors.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_947_arsenic-male-bladder.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1004_synthetic_control.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1056_mc1.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_940_water-treatment.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_950_arsenic-female-lung.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1014_analcatdata_dmft.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1039_hiva_agnostic.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1018_ipums_la_99-small.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1002_ipums_la_98-small.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_765_analcatdata_apnea2.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1016_vowel.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1045_kc1-top5.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1013_analcatdata_challenger.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_450_analcatdata_lawsuit.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_312_scene.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_995_mfeat-zernike.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_764_analcatdata_apnea3.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_311_oil_spill.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_980_optdigits.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_968_analcatdata_birthday.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_987_collins.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_767_analcatdata_apnea1.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_875_analcatdata_chlamydia.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1061_ar4.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_962_mfeat-morphological.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_757_meta.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_951_arsenic-male-lung.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_958_segment.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1064_ar6.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1050_pc3.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_463_backache.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1000_hypothyroid.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1049_pc4.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1023_soybean.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_316_yeast_ml8.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_949_arsenic-female-bladder.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1022_mfeat-pixel.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_954_spectrometer.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_966_analcatdata_halloffame.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_971_mfeat-fourier.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1065_kc3.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1021_page-blocks.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1020_mfeat-karhunen.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_1059_ar1.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_867_visualizing_livestock.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_38_sick.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_984_analcatdata_draft.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_865_analcatdata_neavote.csv: (slice(None, None, None), 0)\n",
      "\n",
      "Failed processing dataset_976_JapaneseVowels.csv: (slice(None, None, None), 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_dir = \"data/raw/class_imbalance\"\n",
    "imputation_strategy = \"mean\"\n",
    "label_encoder = LabelEncoder()\n",
    "        \n",
    "for filename in os.listdir(raw_dir):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "        \n",
    "    file_path = os.path.join(raw_dir, filename)\n",
    "\n",
    "    # Load and validate data\n",
    "    df = pd.read_csv(file_path)\n",
    "    if df.shape[1] < 2:\n",
    "        print(f\"Skipping {filename}: Insufficient columns\")\n",
    "        continue\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.iloc[:, :-1].copy()\n",
    "    y_raw = df.iloc[:, -1].copy()\n",
    "\n",
    "    # Clean and encode labels\n",
    "    valid_mask = y_raw.notna()\n",
    "    X, y_raw = X[valid_mask], y_raw[valid_mask]\n",
    "    y_raw_sorted = np.sort(y_raw)\n",
    "    if len(y_raw_sorted) == 0:\n",
    "        print(f\"Skipping {filename}: No valid targets\")\n",
    "        continue\n",
    "\n",
    "    # Convert labels to 0-indexed integers\n",
    "    y = pd.Series(label_encoder.fit_transform(y_raw_sorted), name=y_raw.name)\n",
    "    classes = label_encoder.classes_\n",
    "    if len(classes) < 2:\n",
    "        print(f\"Skipping {filename}: Only one class present\")\n",
    "        continue\n",
    "\n",
    "    # Handle missing features\n",
    "    if imputation_strategy == \"drop\":\n",
    "        valid_rows = X.notna().all(axis=1)\n",
    "        X, y = X[valid_rows], y[valid_rows]\n",
    "    else:\n",
    "        # Numerical imputation\n",
    "        num_cols = X.select_dtypes(include=np.number).columns\n",
    "        if len(num_cols) > 0:\n",
    "            X[num_cols] = X[num_cols].fillna(X[num_cols].mean()).fillna(0)\n",
    "\n",
    "        # Categorical imputation\n",
    "        cat_cols = X.select_dtypes(exclude=np.number).columns\n",
    "        for col in cat_cols:\n",
    "            mode = X[col].mode()[0] if not X[col].mode().empty else \"missing\"\n",
    "            X[col] = X[col].fillna(mode)\n",
    "\n",
    "    # Convert categoricals to dummies\n",
    "    if len(cat_cols) > 0:\n",
    "        X = pd.get_dummies(X, columns=cat_cols, drop_first=False)\n",
    "\n",
    "        # Final validation\n",
    "        X = X.fillna(0).infer_objects()\n",
    "        assert not X.isna().any().any(), \"NaN values in features\"\n",
    "        assert X.shape[0] > 0, \"Empty dataset after preprocessing\"\n",
    "\n",
    "            # Normalize numericals\n",
    "            num_cols = X.select_dtypes(include=np.number).columns\n",
    "            if len(num_cols) > 0:\n",
    "                X[num_cols] = MinMaxScaler().fit_transform(X[num_cols])\n",
    "\n",
    "        # Stratified split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    tree = DecisionTree(max_depth=5, criterion=\"gini\")\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    predictions = tree.predict(X_test)\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f\"Accuracy: {accuracy}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: dataset_995_mfeat-zernike\n",
      "Accuracy: 0.8733333333333333\n",
      "\n",
      "Processing dataset: dataset_875_analcatdata_chlamydia\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_1061_ar4\n",
      "Accuracy: 0.696969696969697\n",
      "\n",
      "Processing dataset: dataset_1020_mfeat-karhunen\n",
      "Accuracy: 0.9433333333333334\n",
      "\n",
      "Processing dataset: dataset_966_analcatdata_halloffame\n",
      "Accuracy: 0.900497512437811\n",
      "\n",
      "Processing dataset: dataset_765_analcatdata_apnea2\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_1065_kc3\n",
      "Accuracy: 0.8695652173913043\n",
      "\n",
      "Processing dataset: dataset_984_analcatdata_draft\n",
      "Accuracy: 0.990909090909091\n",
      "\n",
      "Processing dataset: dataset_968_analcatdata_birthday\n",
      "Accuracy: 0.9818181818181818\n",
      "\n",
      "Processing dataset: dataset_1021_page-blocks\n",
      "Accuracy: 0.8995127892813642\n",
      "\n",
      "Processing dataset: dataset_450_analcatdata_lawsuit\n",
      "Accuracy: 0.975\n",
      "\n",
      "Processing dataset: dataset_1016_vowel\n",
      "Accuracy: 0.9865319865319865\n",
      "\n",
      "Processing dataset: dataset_1049_pc4\n",
      "Accuracy: 0.8721461187214612\n",
      "\n",
      "Processing dataset: dataset_867_visualizing_livestock\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_1022_mfeat-pixel\n",
      "Accuracy: 0.9366666666666666\n",
      "\n",
      "Processing dataset: dataset_767_analcatdata_apnea1\n",
      "Accuracy: 0.993006993006993\n",
      "\n",
      "Processing dataset: dataset_1045_kc1-top5\n",
      "Accuracy: 0.9545454545454546\n",
      "\n",
      "Processing dataset: dataset_316_yeast_ml8\n",
      "Accuracy: 0.9807162534435262\n",
      "\n",
      "Processing dataset: dataset_949_arsenic-female-bladder\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_950_arsenic-female-lung\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_951_arsenic-male-lung\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_865_analcatdata_neavote\n",
      "Accuracy: 0.9333333333333333\n",
      "\n",
      "Processing dataset: dataset_1004_synthetic_control\n",
      "Accuracy: 0.9388888888888889\n",
      "\n",
      "Processing dataset: dataset_978_mfeat-factors\n",
      "Accuracy: 0.975\n",
      "\n",
      "Processing dataset: dataset_1000_hypothyroid\n",
      "Accuracy: 0.9143109540636042\n",
      "\n",
      "Processing dataset: dataset_1064_ar6\n",
      "Accuracy: 0.7419354838709677\n",
      "\n",
      "Processing dataset: dataset_1059_ar1\n",
      "Accuracy: 0.7837837837837838\n",
      "\n",
      "Processing dataset: dataset_312_scene\n",
      "Accuracy: 0.8132780082987552\n",
      "\n",
      "Processing dataset: dataset_987_collins\n",
      "Accuracy: 0.9933333333333333\n",
      "\n",
      "Processing dataset: dataset_1013_analcatdata_challenger\n",
      "Accuracy: 0.9761904761904762\n",
      "\n",
      "Processing dataset: dataset_954_spectrometer\n",
      "Accuracy: 0.9875\n",
      "\n",
      "Processing dataset: dataset_962_mfeat-morphological\n",
      "Accuracy: 0.8983333333333333\n",
      "\n",
      "Processing dataset: dataset_1039_hiva_agnostic\n",
      "Accuracy: 0.9598108747044918\n",
      "\n",
      "Processing dataset: dataset_980_optdigits\n",
      "Accuracy: 0.8973902728351127\n",
      "\n",
      "Processing dataset: dataset_1050_pc3\n",
      "Accuracy: 0.8848614072494669\n",
      "\n",
      "Processing dataset: dataset_940_water-treatment\n",
      "Accuracy: 0.8805031446540881\n",
      "\n",
      "Processing dataset: dataset_971_mfeat-fourier\n",
      "Accuracy: 0.9066666666666666\n",
      "\n",
      "Processing dataset: dataset_958_segment\n",
      "Accuracy: 0.836940836940837\n",
      "\n",
      "Processing dataset: dataset_1014_analcatdata_dmft\n",
      "Accuracy: 0.8041666666666667\n",
      "\n",
      "Processing dataset: dataset_38_sick\n",
      "Accuracy: 0.9381625441696113\n",
      "\n",
      "Processing dataset: dataset_311_oil_spill\n",
      "Accuracy: 0.9964539007092199\n",
      "\n",
      "Processing dataset: dataset_1002_ipums_la_98-small\n",
      "Accuracy: 0.8931433659839715\n",
      "\n",
      "Processing dataset: dataset_757_meta\n",
      "Accuracy: 0.9874213836477987\n",
      "\n",
      "Processing dataset: dataset_463_backache\n",
      "Accuracy: 0.7962962962962963\n",
      "\n",
      "Processing dataset: dataset_976_JapaneseVowels\n",
      "Accuracy: 0.8651722984275677\n",
      "\n",
      "Processing dataset: dataset_1023_soybean\n",
      "Accuracy: 0.9024390243902439\n",
      "\n",
      "Processing dataset: dataset_764_analcatdata_apnea3\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_1056_mc1\n",
      "Accuracy: 0.9904929577464788\n",
      "\n",
      "Processing dataset: dataset_947_arsenic-male-bladder\n",
      "Accuracy: 1.0\n",
      "\n",
      "Processing dataset: dataset_1018_ipums_la_99-small\n",
      "Accuracy: 0.9321778447626224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data/processed/class_imbalance\"\n",
    "for dataset in os.listdir(base_dir):\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    dataset_path = os.path.join(base_dir, dataset)\n",
    "\n",
    "    # Load preprocessed data\n",
    "    X_train = pd.read_csv(os.path.join(dataset_path, \"X_train.csv\")).values\n",
    "    y_train = pd.read_csv(os.path.join(dataset_path, \"y_train.csv\")).values.flatten()\n",
    "\n",
    "    # Initialize and train model\n",
    "    tree = DecisionTree(max_depth=5, criterion=\"gini\")\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    X_test = pd.read_csv(os.path.join(dataset_path, \"X_test.csv\")).values\n",
    "    predictions = tree.predict(X_test)\n",
    "\n",
    "    # Test    \n",
    "    y_test = pd.read_csv(os.path.join(dataset_path, \"y_test.csv\")).values.flatten()\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f\"Accuracy: {accuracy}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def run_cv(dir,nfolds=10):\n",
    "    results = {}\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=42)\n",
    "    for dataset in os.listdir(dir):\n",
    "        dataset_path = os.path.join(dir, dataset)\n",
    "        X_train = pd.read_csv(os.path.join(dataset_path, \"X_train.csv\")).values\n",
    "        y_train = pd.read_csv(os.path.join(dataset_path, \"y_train.csv\")).values.flatten()\n",
    "        X_test = pd.read_csv(os.path.join(dataset_path, \"X_test.csv\")).values\n",
    "        y_test = pd.read_csv(os.path.join(dataset_path, \"y_test.csv\")).values.flatten()\n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(X_train)):\n",
    "            X_train, X_test = X_train[train_idx], X_test[test_idx]\n",
    "            y_train, y_test = y_train[train_idx], y_test[test_idx]\n",
    "            tree = DecisionTree(max_depth=5, criterion=\"gini\")\n",
    "            tree.fit(X_train, y_train)\n",
    "            y_pred = tree.predict(X_test)\n",
    "            results[dataset].append(accuracy_score(y_test, y_pred))\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 617 is out of bounds for axis 0 with size 600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_original \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mrun_cv\u001b[0;34m(dir, nfolds)\u001b[0m\n\u001b[1;32m     12\u001b[0m y_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, test_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf\u001b[38;5;241m.\u001b[39msplit(X_train)):\n\u001b[0;32m---> 14\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X_train[train_idx], \u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y_train[train_idx], y_test[test_idx]\n\u001b[1;32m     16\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTree(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 617 is out of bounds for axis 0 with size 600"
     ]
    }
   ],
   "source": [
    "results_original = run_cv(base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
